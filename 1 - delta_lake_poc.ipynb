{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf79ac5-bb0a-4a3a-ac22-abb8484838a3",
     "showTitle": true,
     "title": "1 - Instancia absolute paths para as camadas delta lakes"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data-ingestion\").getOrCreate()\n",
    "\n",
    "# paths\n",
    "pathStage = \"dbfs:/FileStore/tables/rpe/stage\"\n",
    "pathBronze = \"dbfs:/FileStore/tables/rpe/bronze\"\n",
    "pathSilver = \"dbfs:/FileStore/tables/rpe/silver\"\n",
    "pathGold = \"dbfs:/FileStore/tables/rpe/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd40d66-588e-4043-ba4f-f2dec2dd4e27",
     "showTitle": true,
     "title": "2- Recupera os conjuntos de dados de uma fonte externa. (Por ex: API, Blob, etc)"
    }
   },
   "outputs": [],
   "source": [
    "customers = (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/customer.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "transactions =  (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/transaction.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "new_customers = (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/new_customer.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "new_transactions = (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/new_transaction.csv\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e168eed3-2496-412a-9508-316fbc6ae928",
     "showTitle": true,
     "title": "3- Guarda os arquivos recuperados em uma landing zone dentro do catalog"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "customers.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/customers.csv\")\n",
    "\n",
    "transactions.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/transactions.csv\")\n",
    "\n",
    "new_customers.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/new_customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/new_customers.csv\")\n",
    "\n",
    "new_transactions.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/new_transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/new_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e84d89-0f48-4f4e-94e2-5bd1ed017d85",
     "showTitle": true,
     "title": "4 - Estruturação do schema das tabelas na camada bronze. A príncipio, sem tratamentos, campos com tipo String como default"
    }
   },
   "outputs": [],
   "source": [
    "customers_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"name\", StringType(), True),\n",
    "\t\tStructField(\"email\", StringType(), True),\n",
    "\t\tStructField(\"signup_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "transactions_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"transaction_id\", StringType(), True),\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"amount\", StringType(), True),\n",
    "\t\tStructField(\"currency\", StringType(), True),\n",
    "        StructField(\"transaction_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "new_customers_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"name\", StringType(), True),\n",
    "\t\tStructField(\"email\", StringType(), True),\n",
    "\t\tStructField(\"signup_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "new_transactions_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"transaction_id\", StringType(), True),\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"amount\", StringType(), True),\n",
    "\t\tStructField(\"currency\", StringType(), True),\n",
    "        StructField(\"transaction_date\", StringType(), True)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf5e9f6-065c-4823-a6ad-984e130038d3",
     "showTitle": true,
     "title": "5 - Carregamentos dos dados na camada bronze"
    }
   },
   "outputs": [],
   "source": [
    "customersBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(customers_schema)\n",
    "    .load(f\"{pathStage}/customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/customers.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "customersBronze.write.format(\"delta\").mode(\"append\").save(f\"{pathBronze}/customers\")\n",
    "\n",
    "transactionsBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(transactions_schema)\n",
    "    .load(f\"{pathStage}/transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/transactions.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "transactionsBronze.write.format(\"delta\").mode(\"append\").save(f\"{pathBronze}/transactions\")\n",
    "\n",
    "newCustomersBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(customers_schema)\n",
    "    .load(f\"{pathStage}/new_customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/new_customers.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "customersBronze.write.format(\"delta\").mode(\"append\").save(f\"{pathBronze}/new_customers\")\n",
    "\n",
    "newTransactionsBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(transactions_schema)\n",
    "    .load(f\"{pathStage}/new_transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/new_transactions.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "transactionsBronze.write.format(\"delta\").mode(\"append\").save(f\"{pathBronze}/new_transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5389be28-5692-4048-81e3-6a580e4f9f56",
     "showTitle": true,
     "title": "6 - Criação da tabelas customers e transations na camada Silver"
    }
   },
   "outputs": [],
   "source": [
    "customersSilver = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(customer_id AS INTEGER) AS customer_id,\n",
    "            CAST(name AS STRING) name,\n",
    "            CAST(email AS STRING) AS email,\n",
    "            CAST(signup_date AS DATE) AS signup_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data\n",
    "       FROM\n",
    "          (\n",
    "            SELECT \n",
    "                DENSE_RANK() OVER(ORDER BY dt_loading_stage DESC) AS rank, * \n",
    "            FROM delta.`{pathBronze}/customers`\n",
    "          ) AS T\n",
    "       WHERE\n",
    "            T.rank = 1\n",
    "       ''')\n",
    ")\n",
    "\n",
    "customersSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"false\").save(f\"{pathSilver}/customers\")\n",
    "\n",
    "\n",
    "transactionsSilver = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(transaction_id AS INTEGER) AS transaction_id,\n",
    "            CAST(customer_id AS INTEGER) customer_id,\n",
    "            CAST(amount AS FLOAT) AS amount,\n",
    "            CAST(currency AS STRING) AS currency,\n",
    "            CAST(transaction_date as DATE) as transaction_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data\n",
    "       FROM\n",
    "          (\n",
    "            SELECT \n",
    "                DENSE_RANK() OVER(ORDER BY dt_loading_stage DESC) AS rank, * \n",
    "            FROM delta.`{pathBronze}/transactions`\n",
    "          ) AS T\n",
    "       WHERE\n",
    "            T.rank = 1\n",
    "       ''')\n",
    ")\n",
    "\n",
    "transactionsSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"false\").save(f\"{pathSilver}/transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6661497c-8308-4035-a6c2-135ff410ba2d",
     "showTitle": true,
     "title": "7- Criação da tabelas newCustomers e newTransations na camada Silver"
    }
   },
   "outputs": [],
   "source": [
    "newCustomersSilver = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(customer_id AS INTEGER) AS customer_id,\n",
    "            CAST(name AS STRING) name,\n",
    "            CAST(email AS STRING) AS email,\n",
    "            CAST(signup_date AS DATE) AS signup_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data\n",
    "       FROM\n",
    "          (\n",
    "            SELECT \n",
    "                DENSE_RANK() OVER(ORDER BY dt_loading_stage DESC) AS rank, * \n",
    "            FROM delta.`{pathBronze}/new_customers`\n",
    "          ) AS T\n",
    "       WHERE\n",
    "            T.rank = 1\n",
    "       ''')\n",
    ")\n",
    "\n",
    "newCustomersSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"false\").save(f\"{pathSilver}/new_customers\")\n",
    "\n",
    "\n",
    "newTransactionsSilver = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(transaction_id AS INTEGER) AS transaction_id,\n",
    "            CAST(customer_id AS INTEGER) customer_id,\n",
    "            CAST(amount AS FLOAT) AS amount,\n",
    "            CAST(currency AS STRING) AS currency,\n",
    "            CAST(transaction_date as DATE) as transaction_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data\n",
    "       FROM\n",
    "          (\n",
    "            SELECT \n",
    "                DENSE_RANK() OVER(ORDER BY dt_loading_stage DESC) AS rank, * \n",
    "            FROM delta.`{pathBronze}/new_transactions`\n",
    "          ) AS T\n",
    "       WHERE\n",
    "            T.rank = 1\n",
    "       ''')\n",
    ")\n",
    "\n",
    "newTransactionsSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"false\").save(f\"{pathSilver}/new_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb283bc-57db-4178-9a21-83d1f072cd4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Disclaimer\n",
    "\n",
    "1- Para fins didáticos, a criação das tabelas na camada **gold** foi feita em tempo de execução junto com suas respectivas cargas. Em um sistema de produção, somente seria executado 1x vez a criação e em cada execução só seria executada a inserção de registros novos e/ou atualização de registros existentes (método upsert).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c16f9a-1e15-409c-b6cc-a7e76c67061b",
     "showTitle": true,
     "title": "8 - Criação da tabela Dim Customer na camada Gold"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "#spark.sql(f''' \n",
    "    #DROP TABLE IF EXISTS delta.`{pathGold}/dim_customer`\n",
    "#''')\n",
    "\n",
    "#spark.sql(f'''\n",
    "    ##CREATE OR REPLACE TABLE delta.`{pathGold}/dim_customer`\n",
    "    ##(\n",
    "    ##    customer_sk long GENERATED ALWAYS AS IDENTITY,\n",
    "    ##    customer_id integer,\n",
    "    ##    name string,\n",
    "    ##    email string,\n",
    "    ##    signup_date date\n",
    "    ##) \n",
    "#''')\n",
    "\n",
    "spark.sql(f''' \n",
    "     MERGE INTO delta.`{pathGold}/dim_customer` dest\n",
    "        USING (\n",
    "             select \n",
    "                c.customer_id as customer_id,\n",
    "                c.name,\n",
    "                c.email,\n",
    "                c.signup_date\n",
    "                from delta.`{pathSilver}/customers` c\n",
    "        ) orig\n",
    "        ON orig.customer_id = dest.customer_id\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            dest.customer_id = orig.customer_id,\n",
    "            dest.name        = orig.name,\n",
    "            dest.email       = orig.email,\n",
    "            dest.signup_date = orig.signup_date\n",
    "        WHEN NOT MATCHED\n",
    "        THEN INSERT (\n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            signup_date\n",
    "        )\n",
    "        VALUES (\n",
    "            orig.customer_id,\n",
    "            orig.name,\n",
    "            orig.email,\n",
    "            orig.signup_date\n",
    "        )         \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8536b2c4-76f5-4633-9f93-3cff1d4f5de3",
     "showTitle": true,
     "title": "9 - 8 - Criação da tabela Fat Sales na camada Gold"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "##spark.sql(f''' \n",
    "    ##DROP TABLE IF EXISTS delta.`{pathGold}/fat_sales`\n",
    "##''')\n",
    "\n",
    "##spark.sql(f'''\n",
    "    ##CREATE OR REPLACE TABLE delta.`{pathGold}/fat_sales`\n",
    "    ##(\n",
    "        ##transaction_sk long GENERATED ALWAYS AS IDENTITY,\n",
    "        ##customer_sk integer,\n",
    "        ##transaction_id integer,\n",
    "        ##amount_sale string,\n",
    "        ##transaction_date date\n",
    "    ##) \n",
    "    ##USING DELTA PARTITIONED BY (transaction_date) \n",
    "##''')\n",
    "\n",
    "spark.sql(f'''   \n",
    "        insert into delta.`{pathGold}/fat_sales`\n",
    "        (\n",
    "            customer_sk,\n",
    "            transaction_id,\n",
    "            amount_sale,\n",
    "            transaction_date\n",
    "        )\n",
    "        select \n",
    "            dc.customer_sk,\n",
    "            t.transaction_id,\n",
    "            case t.currency\n",
    "            when 'USD' then concat('$ ', cast(t.amount as string))\n",
    "            when 'BRL' then concat('R$ ', cast(t.amount as string))\n",
    "            when 'EUR' then concat('Ç', cast(t.amount as string))\n",
    "            else 'NA'\n",
    "            end as amount_sale,\n",
    "            t.transaction_date\n",
    "        from delta.`{pathSilver}/transactions` t \n",
    "        join delta.`{pathSilver}/customers` c on c.customer_id = t.customer_id\n",
    "        join delta.`{pathGold}/dim_customer` dc on dc.customer_id = c.customer_id\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2427472522797994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - delta_lake_poc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
