{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acc737c-8943-4079-a4b7-e3e4f135c79b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Disclaimer \n",
    "\n",
    "A estratégia da carga FULL se torna mais fácil na maioria dos cenários, pois o procedimento padrão é replicar os dados da origem atráves do processo de ETL sem ter a necessidade de gerar um histórico de alterações de dados. O método padrão é TRUNCATE INSERT. \n",
    "\n",
    "Como boa prática, adicionamos uma coluna de timestamp para guardamos o dia/hora da última carga na tabela.  \n",
    "\n",
    "Refs. \n",
    "\n",
    "https://www.linkedin.com/pulse/data-load-strategies-full-vs-incremental-janardhan-reddy-kasireddy/\n",
    "\n",
    "https://mesum.medium.com/data-warehousing-historical-load-full-load-incremental-load-cb46f0d0c4f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef72594-9592-4d2e-bfef-ab5bf3c427ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data-ingestion\").getOrCreate()\n",
    "\n",
    "# paths\n",
    "pathStage = \"dbfs:/FileStore/tables/rpe/stage\"\n",
    "pathBronze = \"dbfs:/FileStore/tables/rpe/bronze\"\n",
    "pathSilver = \"dbfs:/FileStore/tables/rpe/silver\"\n",
    "pathGold = \"dbfs:/FileStore/tables/rpe/gold\"\n",
    "\n",
    "\n",
    "## External data source\n",
    "customers = (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/customer.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "transactions =  (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/transaction.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "customers.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/customers.csv\")\n",
    "\n",
    "transactions.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/transactions.csv\")\n",
    "\n",
    "## define schema\n",
    "customers_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"name\", StringType(), True),\n",
    "\t\tStructField(\"email\", StringType(), True),\n",
    "\t\tStructField(\"signup_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "transactions_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"transaction_id\", StringType(), True),\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"amount\", StringType(), True),\n",
    "\t\tStructField(\"currency\", StringType(), True),\n",
    "        StructField(\"transaction_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "## copy \"as is\" - landing para bronze\n",
    "customersBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(customers_schema)\n",
    "    .load(f\"{pathStage}/customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/customers.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "customersBronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{pathBronze}/customers\")\n",
    "\n",
    "transactionsBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(transactions_schema)\n",
    "    .load(f\"{pathStage}/transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/transactions.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "transactionsBronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{pathBronze}/transactions\")\n",
    "\n",
    "## copy \"as is\" - bronze para silver\n",
    "\n",
    "spark.sql(f'''\n",
    "    TRUNCATE TABLE delta.`{pathSilver}/customers`\n",
    "''')\n",
    " \n",
    "\n",
    "customersSilver = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(customer_id AS INTEGER) AS customer_id,\n",
    "            CAST(name AS STRING) name,\n",
    "            CAST(email AS STRING) AS email,\n",
    "            CAST(signup_date AS DATE) AS signup_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data \n",
    "        FROM delta.`{pathBronze}/customers`\n",
    "    ''')\n",
    ")\n",
    "\n",
    "customersSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{pathSilver}/customers\")\n",
    "\n",
    "spark.sql(f'''\n",
    "    TRUNCATE TABLE delta.`{pathSilver}/transactions`\n",
    "''')\n",
    " \n",
    "transactionsSilver = (\n",
    "    spark.sql(f'''\n",
    "      SELECT\n",
    "            CAST(transaction_id AS INTEGER) AS transaction_id,\n",
    "            CAST(customer_id AS INTEGER) customer_id,\n",
    "            CAST(amount AS FLOAT) AS amount,\n",
    "            CAST(currency AS STRING) AS currency,\n",
    "            CAST(transaction_date as DATE) as transaction_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data \n",
    "        FROM delta.`{pathBronze}/transactions`\n",
    "''')\n",
    ")\n",
    "\n",
    "transactionsSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{pathSilver}/transactions\")\n",
    "\n",
    "\n",
    "## Copy \"as is\" - silver para gold\n",
    "\n",
    "spark.sql(f'''\n",
    "    TRUNCATE TABLE delta.`{pathGold}/dim_customer`\n",
    "''')\n",
    " \n",
    "dim_customer = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(customer_id AS INTEGER) AS customer_id,\n",
    "            CAST(name AS STRING) name,\n",
    "            CAST(email AS STRING) AS email,\n",
    "            CAST(signup_date AS DATE) AS signup_date,\n",
    "            CAST(dt_insert_data AS TIMESTAMP) AS dt_insert_data \n",
    "        FROM delta.`{pathSilver}/customers`\n",
    "    ''')\n",
    ")\n",
    "\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{pathGold}/dim_customer\")\n",
    "\n",
    "spark.sql(f'''\n",
    "    TRUNCATE TABLE delta.`{pathGold}/fat_sales`\n",
    "''')\n",
    "\n",
    "spark.sql(f'''   \n",
    "        insert into delta.`{pathGold}/fat_sales`\n",
    "        (\n",
    "            customer_sk,\n",
    "            transaction_id,\n",
    "            amount_sale,\n",
    "            transaction_date\n",
    "        )\n",
    "        select \n",
    "            dc.customer_sk,\n",
    "            t.transaction_id,\n",
    "            case t.currency\n",
    "            when 'USD' then concat('$ ', cast(t.amount as string))\n",
    "            when 'BRL' then concat('R$ ', cast(t.amount as string))\n",
    "            when 'EUR' then concat('Ç', cast(t.amount as string))\n",
    "            else 'NA'\n",
    "            end as amount_sale,\n",
    "            t.transaction_date\n",
    "        from delta.`{pathSilver}/transactions` t \n",
    "        join delta.`{pathSilver}/customers` c on c.customer_id = t.customer_id\n",
    "        join delta.`{pathGold}/dim_customer` dc on dc.customer_id = c.customer_id\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - carga_full_poc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
