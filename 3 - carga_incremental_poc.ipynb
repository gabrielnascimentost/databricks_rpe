{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474c1fa5-9449-4c71-881a-3620e3bdc78a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Disclaimer \n",
    "\n",
    "A estratégia da carga incremental segue o modelo exposto no notebook **1 - delta_lake_poc**, onde se tem um landing zone que captura dados continuamente de fontes externas, uma **camada bronze** (\"as is\") como sendo a primeira camada que armazena todos os dados contidos na landing zone com adição de uma coluna \"flag\" de inserção. Nesta camada, temos um histórico bruto de todas as ingestões e conseguimos traçar uma linhagem dos dados por meio de análise de dados, se preciso. Avançando, temos a **camada silver** que traz um overview ou snapshot da última ingestão de dados, ou seja, é uma camada de preparo para posterior carga na camada gold. Nesta camada, aplicamos transformações, tipagem de campos e regras de negócio (caso necessário). Por fim, temos a **camada gold**, camada alta e refinada que traz o produto de dados pronto, tabelas a nível de negócio que trazem respostas confiáveis para o cliente.\n",
    "\n",
    "PS:\n",
    "\n",
    "1- A depender da necessidade do cliente, pode-se aplicar algum dos 6 tipos de Slowly Changing Dimensions (SCD), contudo, para fins didáticos, deixaremos de lado este recurso.\n",
    "\n",
    "2- A depender da fonte de dados externa, podemos implementar o método de Streaming DataFrame do Apache Spark. Por meio de leituras continuas e com ou sem intervalos programados, é possível ir fazendo o incremento automático da fonte de dados de origem para destino. Por exemplo, semmpre que surgir um dado novo na landing zone, o fluxo de carga (bronze, silver e gold) é iniciado. Pode-se também utilizar Apache Kafka como conector externo. Aliada a estratégia de carga incremental, seja batch ou streaming, pode-se utilizar Delta Live Tables (DLT).   \n",
    "\n",
    "Refs. \n",
    "\n",
    "https://www.linkedin.com/pulse/data-load-strategies-full-vs-incremental-janardhan-reddy-kasireddy/\n",
    "\n",
    "https://mesum.medium.com/data-warehousing-historical-load-full-load-incremental-load-cb46f0d0c4f5\n",
    "\n",
    "https://medium.com/@sharankarthick487/incremental-data-load-approaches-overview-11c90fd87fe6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd9535d-7fb1-42bc-8662-b65ade5c44cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data-ingestion\").getOrCreate()\n",
    "\n",
    "# paths\n",
    "pathStage = \"dbfs:/FileStore/tables/rpe/stage\"\n",
    "pathBronze = \"dbfs:/FileStore/tables/rpe/bronze\"\n",
    "pathSilver = \"dbfs:/FileStore/tables/rpe/silver\"\n",
    "pathGold = \"dbfs:/FileStore/tables/rpe/gold\"\n",
    "\n",
    "\n",
    "## External data source\n",
    "customers = (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/customer.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "transactions =  (\n",
    "    spark.createDataFrame(\n",
    "        pandas.read_csv(\"https://raw.githubusercontent.com/gabrielnascimentost/databricks_rpe/main/database/raw/transaction.csv\")\n",
    "    )\n",
    ")\n",
    "\n",
    "customers.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/customers.csv\")\n",
    "\n",
    "transactions.write.format(\"csv\").mode(\"overwrite\").save(f\"{pathStage}/transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/transactions.csv\")\n",
    "\n",
    "## define schema\n",
    "customers_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"name\", StringType(), True),\n",
    "\t\tStructField(\"email\", StringType(), True),\n",
    "\t\tStructField(\"signup_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "transactions_schema = (\n",
    "    StructType([\n",
    "\t\tStructField(\"transaction_id\", StringType(), True),\n",
    "\t\tStructField(\"customer_id\", StringType(), True),\n",
    "\t\tStructField(\"amount\", StringType(), True),\n",
    "\t\tStructField(\"currency\", StringType(), True),\n",
    "        StructField(\"transaction_date\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "## copy \"as is\" - landing para bronze\n",
    "customersBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(customers_schema)\n",
    "    .load(f\"{pathStage}/customers/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/customers.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "customersBronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{pathBronze}/customers\")\n",
    "\n",
    "transactionsBronze = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(transactions_schema)\n",
    "    .load(f\"{pathStage}/transactions/{datetime.datetime.now().year}/{datetime.datetime.now().month}/{datetime.datetime.now().day}/transactions.csv\")\n",
    "    .withColumn(\"dt_loading_stage\", current_timestamp())\n",
    ")\n",
    "\n",
    "transactionsBronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{pathBronze}/transactions\")\n",
    "\n",
    "## copy - bronze para silver filtrando somente o registro mais atual\n",
    "\n",
    "customersSilver = (\n",
    "    spark.sql(f'''\n",
    "       SELECT\n",
    "            CAST(customer_id AS INTEGER) AS customer_id,\n",
    "            CAST(name AS STRING) name,\n",
    "            CAST(email AS STRING) AS email,\n",
    "            CAST(signup_date AS DATE) AS signup_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data\n",
    "       FROM\n",
    "          (\n",
    "            SELECT \n",
    "                DENSE_RANK() OVER(ORDER BY dt_loading_stage DESC) AS rank, * \n",
    "            FROM delta.`{pathBronze}/customers`\n",
    "          ) AS T\n",
    "       WHERE\n",
    "            T.rank = 1\n",
    "    ''')\n",
    ")\n",
    "\n",
    "customersSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{pathSilver}/customers\")\n",
    "\n",
    "transactionsSilver = (\n",
    "    spark.sql(f'''\n",
    "     SELECT\n",
    "            CAST(transaction_id AS INTEGER) AS transaction_id,\n",
    "            CAST(customer_id AS INTEGER) customer_id,\n",
    "            CAST(amount AS FLOAT) AS amount,\n",
    "            CAST(currency AS STRING) AS currency,\n",
    "            CAST(transaction_date as DATE) as transaction_date,\n",
    "            CAST(dt_loading_stage AS TIMESTAMP) AS dt_insert_data\n",
    "       FROM\n",
    "          (\n",
    "            SELECT \n",
    "                DENSE_RANK() OVER(ORDER BY dt_loading_stage DESC) AS rank, * \n",
    "            FROM delta.`{pathBronze}/transactions`\n",
    "          ) AS T\n",
    "       WHERE\n",
    "            T.rank = 1\n",
    "''')\n",
    ")\n",
    "\n",
    "transactionsSilver.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(f\"{pathSilver}/transactions\")\n",
    "\n",
    "\n",
    "## Copy - silver para gold fazendo upsert (insert e update)\n",
    "\n",
    "spark.sql(f'''\n",
    "       MERGE INTO delta.`{pathGold}/dim_customer` dest\n",
    "        USING (\n",
    "             select \n",
    "                c.customer_id as customer_id,\n",
    "                c.name,\n",
    "                c.email,\n",
    "                c.signup_date\n",
    "                from delta.`{pathSilver}/customers` c\n",
    "        ) orig\n",
    "        ON orig.customer_id = dest.customer_id\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            dest.customer_id = orig.customer_id,\n",
    "            dest.name        = orig.name,\n",
    "            dest.email       = orig.email,\n",
    "            dest.signup_date = orig.signup_date\n",
    "        WHEN NOT MATCHED\n",
    "        THEN INSERT (\n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            signup_date\n",
    "        )\n",
    "        VALUES (\n",
    "            orig.customer_id,\n",
    "            orig.name,\n",
    "            orig.email,\n",
    "            orig.signup_date\n",
    "        )         \n",
    "''')\n",
    "\n",
    "spark.sql(f'''   \n",
    "        insert into delta.`{pathGold}/fat_sales`\n",
    "        (\n",
    "            customer_sk,\n",
    "            transaction_id,\n",
    "            amount_sale,\n",
    "            transaction_date\n",
    "        )\n",
    "        select \n",
    "            dc.customer_sk,\n",
    "            t.transaction_id,\n",
    "            case t.currency\n",
    "            when 'USD' then concat('$ ', cast(t.amount as string))\n",
    "            when 'BRL' then concat('R$ ', cast(t.amount as string))\n",
    "            when 'EUR' then concat('Ç', cast(t.amount as string))\n",
    "            else 'NA'\n",
    "            end as amount_sale,\n",
    "            t.transaction_date\n",
    "        from delta.`{pathSilver}/transactions` t \n",
    "        join delta.`{pathSilver}/customers` c on c.customer_id = t.customer_id\n",
    "        join delta.`{pathGold}/dim_customer` dc on dc.customer_id = c.customer_id\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3 - carga_incremental_poc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
